---
title: "Evaluation of classifier for detecting the correct performance of barbell lifts"
author: "Gonzalo Bailador del Pozo"
date: "21 de mayo de 2017"
output: html_document
---
```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis
In this analysis, I have evaluated the feasibility of using a classifier for detecting the correct perfomance of barbell lifts. 

## Loading data

First of all, I loaded the data which is stored in two csv files one for training and other testing. Besides, I labelled as NA(Not Available) those samples with the strings "NA","NaN","" and "#DIV/0!".

```{r Loading libraries,echo=FALSE,include=FALSE}
library(MASS)
library(lattice)
library(ggplot2)
library(randomForest)
library(caret)
```

```{r Loading data}
training<-read.csv("pml-training.csv",na.strings = c("NA","NaN","","#DIV/0!"))
testing<-read.csv("pml-testing.csv",na.strings = c("NA","NaN","","#DIV/0!"))
```

## Preprocessing data
I removed the columns that contain NA samples and some columns that do not provide information about the movement but information about user, window and timestamps.

```{r Removing NA}
removeCols<-colSums(is.na(training))>0
removeCols<-names(removeCols[removeCols])
removeCols<-c(removeCols,'X','user_name')
removeCols<-c(removeCols,'raw_timestamp_part_1','raw_timestamp_part_2')
removeCols<-c(removeCols,'cvtd_timestamp','new_window','num_window')
training<-training[,setdiff(names(training),removeCols)]
testing<-testing[,setdiff(names(testing),removeCols)]
```
## Exploratory analysis of the data
In order to know if the different classes are balanced in the dataset, we calculate the histogram of the feature class for the training set.
```{r Histogram of the different classes}
hist(as.numeric(training$classe), xlab="Class",xaxt="n")
axis(1,at=1:5,labels=letters[1:5])
``` 
This histograms shows that there is no high unbalancement of the classes only the first one has a bit more samples than the others.

Using a boxplot, the ranges of the different parameters are represented in order to decide if applying a normalization stage 

```{r Analysis of the range of  features,echo=FALSE}
boxplot(training)
``` 
This boxplot shows that the features have different ranges so I applied a normalization stage

## Normalize data
Since the different columns can present different ranges, I normalized the signals by substracting the mean and dividing by the standard deviation. These parameters have been calculated using the training set and they are applied to the training and test set.

```{r Normalization of  data, echo=TRUE}
preProcValues <- preProcess(training, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, training)
testTransformed <- predict(preProcValues, testing)
``` 
## Classifying data
After the normalization, I tested one of the simplest classifiers (Linear Discriminant Analysis) in order to obtain a reference rate. For this test, we use a 10-fold crossvalidation for assessing how the results will generalize to an independent data set. NOTE: I commented the creation of the models because it takes really long time so I load them directly
```{r Classifying Data using LDA, echo=TRUE}
load('models.RData')
train_control <- trainControl(method="cv", number=10)
#modelLDA<-train(classe~.,data=trainTransformed,trainControl=train_control,method="lda")
confusionMatrix(modelLDA)
```
Since this method does not provide high performance only about (70%), I tested other methods based on ensemble classifiers. Concretely, I used Random Forest.

```{r Classifying Data using Random Forest, echo=TRUE}
train_control <- trainControl(method="cv", number=10)
#modelRF<-train(classe~.,data=trainTransformed,trainControl=train_control,method="rf")
confusionMatrix(modelRF)
```
This method achieved a high accuracy (90%) so I selected this method for predicting the new data. 

## Predicting New Data

We predict the new data using the best predictor which was obtained using "Random Forest" and we obtain the following predictions:
```{r Prediction of testing data, echo=TRUE}
predictions<-predict(modelRF,newdata=testing)
plot(1:length(predictions),predictions,xlab="Predictions",ylab="Class", yaxt="n")
axis(2,at=1:5,labels=letters[1:5])
predictions
```
